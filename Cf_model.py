# -*- coding: utf-8 -*-
"""FInal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QKSotObgFjioBtKVhaCoDCV4UifDOI-A
"""



import numpy as np
import pandas as pd
from scipy.stats import randint
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error, r2_score, mean_absolute_error
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC # Added this import
import warnings
warnings.filterwarnings('ignore')

"""# **Task 1** : Answer"""

CLF_PATH = "/content/Loan.csv"
target_clf = "Loan_Status"

df = pd.read_csv(CLF_PATH)

print("Shape:", df.shape)
display(df.head())

counts = df[target_clf].value_counts()
print("\nClass counts:\n", counts)

from ydata_profiling import ProfileReport

profile = ProfileReport(df, title="Pandas Profiling Report", explorative=True)
profile.to_file("Loan_report.html")

"""check the sitution of the dataset

# **Task 2:** Answer
"""

sns.boxplot(df)
plt.show()

"""we can see the out layers here"""

X = df.drop(columns=[target_clf])
y = df[target_clf]

numeric_features = X.select_dtypes(include=["int64", "float64"]).columns
categorical_features = X.select_dtypes(include=["object"]).columns

print("Numeric:", list(numeric_features))
print("Categorical:", list(categorical_features))

"""Here we separated the dataset into features (X) and target (y): X =  numeric Y = categorical"""

if 'Loan_ID' in df.columns:
    df = df.drop('Loan_ID', axis=1)

X = df.drop(target_clf, axis=1)
y = df[target_clf]

"""Droped the Loan_ID"""

X = pd.get_dummies(X, drop_first=True)


numeric_features_list = X.columns.tolist()

num_transfrom = Pipeline(
    steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ]
)

preprocessor = ColumnTransformer(
    transformers=[
        ('num', num_transfrom, numeric_features_list)
    ]
)

print("Encoded X shape:", X.shape)

"""has missing value so handled it here and use one hot encoded

# **Task 3:** Answer
"""

X_train,X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2 , random_state=42, stratify = y)

c_lr = LogisticRegression(random_state=42)
c_svm = SVC(kernel='rbf', C=1.0, random_state=42)
c_rf = RandomForestClassifier(n_estimators=100, random_state=42)
c_gb = GradientBoostingClassifier(n_estimators=100, random_state=42)


voting_clf = VotingClassifier(
    estimators=[
        ('rf', c_rf),
        ('gb', c_gb)
    ],
)

staking_clf = StackingClassifier(
    estimators=[
        ('rf', c_rf),
        ('gb', c_gb)
    ],
    final_estimator = LogisticRegression()
)


model_to_train = {
    'Logistic Regression': c_lr,
    'SVM (RBF)': c_svm,
    'Random Forest': c_rf,
    'Voting Classifier': voting_clf,
    'Stacking Classifier': staking_clf
}

result = []

for name, model in model_to_train.items():
    pipe = Pipeline(
        [
            ('preprocessor', preprocessor),
            ('model', model)
        ]
    )
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    result.append([name, accuracy])

result_df = pd.DataFrame(result, columns=['Model', 'Accuracy']).sort_values(by='Accuracy', ascending=False)
result_df

"""# **Task 4 and 5** Answer"""

best_model = result_df.iloc[0]['Model']
best_model_obj = model_to_train[best_model]

final_pipe = Pipeline(
    [
        ('preprocessor', preprocessor),
        ('model', best_model_obj)
    ]
)

final_pipe.fit(X_train, y_train)
final_y_pred = final_pipe.predict(X_test)

print("Best Model:", best_model)
print("Accuracy:", accuracy_score(y_test, final_y_pred))

"""best model Logistic Regression

# **Task 6** Answer
"""

pipeline = Pipeline(
    [
        ('preprocessor', preprocessor),
        ('model', best_model_obj)
    ]
)

cv_scores = cross_val_score(
    pipeline,
    X_train,
    y_train,
    cv=10,
    scoring='accuracy'
)

print("Cross-validation scores: ", cv_scores)
print("Mean accuracy: ", cv_scores.mean())
print("Standard deviation of accuracy:", cv_scores.std())

"""# **Task 7  and 8**: Answer"""



"""# **Task 9** Answer"""

rf_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', c_rf)
])

para_dist = {
    'model__n_estimators': randint(50, 200),
    'model__max_depth': randint(5, 26),
    'model__min_samples_split': randint(2, 10)
}

grid_search = RandomizedSearchCV(
    estimator=rf_pipeline,
    param_distributions=para_dist,
    n_iter=50,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

grid_search.fit(X_train, y_train)

y_evaluation = grid_search.predict(X_test)

print("Best Params:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

y_evaluation = grid_search.predict(X_test)
print(classification_report(y_test, y_evaluation))

import pickle

with open ("Loan.pkl", "wb") as f:
    pickle.dump(grid_search, f)